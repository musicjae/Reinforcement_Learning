{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy_iteration.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPzHJ/i3k0xOTnRwpfZ9eiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musicjae/Reinforcement_Learning/blob/master/policy_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2xgBWhRD5TG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 컴퓨터 내에서 특정 파일 마운트 하기\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugrKbKF1EmFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 구글 드라이브 전체와 마운트 하기\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0q821fIEWnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrC4Fq25HYxU",
        "colab_type": "text"
      },
      "source": [
        " **Intro**<br><br>\n",
        " &nbsp; &nbsp;보통 The Bellman equation은 dynamic programming (policy_iteration, value_iteration)을 통해 풀 수 있다. 이번 실행에서 우리는 Bellman eq를 사용해 policy iteration을 풀어볼 것이다.\n",
        "<br><br>\n",
        " **Assumption**<br><br>\n",
        "\n",
        " &nbsp; &nbsp; (1) The optimal policy is not random. We need to develop the present policy which is called Policy Improvement. <br>\n",
        " &nbsp; &nbsp;(2) By implemetation of policy evaluation, we should develop the present Improvement. Thus, we need to consider what is the policy evaluation.<br><br>\n",
        " **Policy Evaluation**\n",
        "\n",
        "&nbsp; &nbsp;PE? 특정 정책이 얼마나 좋은지에 대해 평가하기. <br>\n",
        "&nbsp; &nbsp;Policy value? 현재 정책에 따라 받을 보상에 대한 정보.<Br><Br><Br>\n",
        "$$(1)\\space\\space v_{\\pi}(s) = E_{\\pi}[R_{t+1}+ \\gamma R_{t+2}+ (\\gamma)^2R_{t+3}+ ... | S_t = s]$$\n",
        "<br><br>\n",
        " 하지만 컴퓨터는 위 식을 계산하지 못한다. 우리는 이것을 계산 가능한 식으로 바꿔줘야 한다. 그 식은 다음과 같다:<br><br>\n",
        " $$(2)\\space\\space  v_{\\pi}(s) = \\sum_{a∈A}\\pi(a|s)(r_{(s,a)} + \\gamma v_{\\pi}(S'))$$\n",
        "<br><br>\n",
        "정책 평가 PE 는 위 식을 반복적으로 수행하는 것이다. 우리가 $\\pi$를  새로운 변수 $k=1,2, ...$로 표현할 때, $k+1$ 번째 가치함수에 대한 방정식은 다음과 같다:<br><br>\n",
        "$$(3)\\space\\space v_{k+1}(s) = \\sum_{a∈A}\\pi(a|s)(r_{(s,a)} + \\gamma v_{k}(S'))$$\n",
        "\n",
        "**Details of (3):**<br><br>\n",
        "- k th의 가치함수 행렬에서 현재 상태 s에서 갈 수 있는 다음 상태 s' 에 저장된 가치 함수 $v_k(s')$ 불러오기\n",
        "-  그 가치 함수에 할인율을 곱한 뒤, 그 상태로 가는 행동에 대한 보상 R을 더한다:<Br>\n",
        "$$(4)\\space\\space r_{(s,a)} + \\gamma v_k(s')$$<Br>\n",
        "- (4)에다가 그 행동을 할 확률 (정책) 을 곱한다:<br><Br>\n",
        "$$(5)\\space\\space \\pi(a|s)(r_{(s,a)} + \\gamma v_k(s'))$$<Br>\n",
        "- (5)를 ALL 선택 가능한 행동에 대해 반복하고 그 값들을 더해준다:<br><br>\n",
        "$$(6)\\space\\space \\sum_{a∈A}\\pi(a|s)r_{(s,a)} + \\gamma v_k(s')$$<Br>\n",
        "- (6) 과정을 통해 얻은 값을 k+1 th 가치 함수 행렬의 상태 s 자리에 저장해준다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOihEukIEfig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from environment import GraphicDisplay, Env\n",
        "\n",
        "\n",
        "class PolicyIteration:\n",
        "    def __init__(self, env):\n",
        "        # 환경에 대한 객체 선언\n",
        "        self.env = env\n",
        "        # 가치함수를 2차원 리스트로 초기화\n",
        "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
        "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
        "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
        "                             for _ in range(env.height)]\n",
        "        # 마침 상태의 설정\n",
        "        self.policy_table[2][2] = []\n",
        "        # 할인율\n",
        "        self.discount_factor = 0.9\n",
        "\n",
        "    # 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가\n",
        "    def policy_evaluation(self):\n",
        "        # 다음 가치함수 초기화\n",
        "        next_value_table = [[0.00] * self.env.width\n",
        "                            for _ in range(self.env.height)]\n",
        "\n",
        "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
        "        for state in self.env.get_all_states(): # 에이전트는 가능한 한 모든 상태에 대해 알아야 하니까\n",
        "            value = 0.0\n",
        "            # 마침 상태의 가치 함수 = 0\n",
        "            if state == [2, 2]:\n",
        "                next_value_table[state[0]][state[1]] = value\n",
        "                continue\n",
        "\n",
        "            # 벨만 기대 방정식\n",
        "            for action in self.env.possible_actions: # 에이전트의 모든 행동\n",
        "                next_state = self.env.state_after_action(state, action) # 에이전트가 특정 상태에서 특정 행동을 할 때, 다음 상태에 대한 환경 정보\n",
        "                reward = self.env.get_reward(state, action) # 환경이 주는 보상\n",
        "                next_value = self.get_value(next_state)\n",
        "                value += (self.get_policy(state)[action] * \n",
        "                          (reward + self.discount_factor * next_value))\n",
        "\n",
        "            next_value_table[state[0]][state[1]] = value\n",
        "\n",
        "        self.value_table = next_value_table\n",
        "\n",
        "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
        "    def policy_improvement(self):\n",
        "        next_policy = self.policy_table\n",
        "        for state in self.env.get_all_states():\n",
        "            if state == [2, 2]:\n",
        "                continue\n",
        "\n",
        "            value_list = []\n",
        "            # 반환할 정책 초기화\n",
        "            result = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "            # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n",
        "            for index, action in enumerate(self.env.possible_actions):\n",
        "                next_state = self.env.state_after_action(state, action)\n",
        "                reward = self.env.get_reward(state, action)\n",
        "                next_value = self.get_value(next_state)\n",
        "                value = reward + self.discount_factor * next_value\n",
        "                value_list.append(value)\n",
        "\n",
        "            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n",
        "            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
        "            max_idx_list = max_idx_list.flatten().tolist()\n",
        "            prob = 1 / len(max_idx_list)\n",
        "\n",
        "            for idx in max_idx_list:\n",
        "                result[idx] = prob\n",
        "\n",
        "            next_policy[state[0]][state[1]] = result\n",
        "\n",
        "        self.policy_table = next_policy\n",
        "\n",
        "    # 특정 상태에서 정책에 따라 무작위로 행동을 반환\n",
        "    def get_action(self, state):\n",
        "        policy = self.get_policy(state)\n",
        "        policy = np.array(policy)\n",
        "        return np.random.choice(4, 1, p=policy)[0]\n",
        "\n",
        "    # 상태에 따른 정책 반환\n",
        "    def get_policy(self, state):\n",
        "        return self.policy_table[state[0]][state[1]]\n",
        "\n",
        "    # 가치 함수의 값을 반환\n",
        "    def get_value(self, state):\n",
        "        return self.value_table[state[0]][state[1]]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = Env() # 에이전트에게 환경에 대한 정보가 필요하니까 env를 객체로 생성\n",
        "    policy_iteration = PolicyIteration(env) #이 env를 정책 반복 클래스의 인수로 전달 -> 환경의 env() 클래스에 접근 가능해져\n",
        "    grid_world = GraphicDisplay(policy_iteration)\n",
        "    grid_world.mainloop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}